---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<figure style="width: 40%;margin-bottom: 0px;margin-top: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/IMG_8446.jpg" alt=""> </figure>  
**About**
<span style="font-size:90%">  
*I'm a research scientist at NXN Labs, where I'm developing large-scale generative foundation models for fashion imagery. My research interest lies in image/video generation and 3D computer vision, with the broader goal of modeling and interacting with the visual world. Prior to NXN Labs, I was at Innerverz AI, focusing on video diffusion models. I earned my PhD from Korea University, advised by Prof. Hanseok Ko. I also complete my B.S and M.S degrees in Electrical Engineering from Korea University.*  
</span>  
<span style="font-size:90%">    
[[Google Scholar](https://scholar.google.co.kr/citations?user=OokxUB4AAAAJ)]  [[Github](https://github.com/jgkwak95)]  [[CV]({{ site.url }}{{ site.baseurl }}/files/CV_jgkwak.pdf)]  
</span>
### News 
<span style="font-size:80%">   
**[Jul. 2025] In September, I will start a postdoctoral fellow position at the University of British Columbia (UBC), working with Prof. <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a>.**     
**[Jun. 2025] I've been recognized as an <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer">Outstanding Reviewer</a> at CVPR 2025 (711/12,593 reviewers)**   
**[May 2025] One paper has been accepted to CVPRW 2025.**   
**[Dec. 2024] I've joined <a href="https://nxn.ai/">NXN Labs</a> as an AI researcher, focusing on visual generative/editing models.**    
**[Jul. 2024] I will be giving a talk at <a href="https://www.twelvelabs.io/">Twelve Labs</a>.**   
**[Apr. 2024] Our <a href="https://arxiv.org/abs/2312.01305">paper</a> has been selected as one of <a href="https://public.tableau.com/views/CVPR2024/CVPRtrends?:showVizHome=no">Highlight Papers</a> at CVPR 2024 (Top 10%).**  
**[Feb. 2024] One paper has been accepted to <a href="https://cvpr.thecvf.com">CVPR 2024</a>.**  
**[Jan. 2024] One paper has been accepted to <a href="https://2024.ieeeicassp.org/">ICASSP 2024</a>.**  
**[Jan. 2024] I've joined  <a href="https://innerverz.io/">Innerverz AI</a> as an AI/ML researcher, focusing on video diffusion models.**  
**[Dec. 2023] I've successfully defended my thesis, "Towards Controllable and Interpretable Generative Neural Rendering".**  
**[Dec. 2023] I completed a 6-month visit to the University of British Columbia (UBC) as a PhD visiting student in the Computer Vision Lab, supervised by Prof. <a href="https://www.cs.ubc.ca/~kmyi/">Kwang Moo Yi</a>.**  
</span>

Selected Publications
======
<figure style="width: 80%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/vivid_homepage.png" alt=""> </figure>| <span style="font-size:110%">ViVid-1-to-3:
Novel View Synthesis with Video Diffusion Models</span><br/>
<span style="font-size:90%">**Jeong-gi Kwak \*** , <span style="color: #808080">Erqun Dong \*, Yuhe Jin,  Hanseok Ko, Shweta Mahajan, Kwang Moo Yi </span></span><br/>
*IEEE/CVF Conference on Computer Vision and Pattern Recognition (**CVPR**), 2024*<br>
<span font style="font-size:90%;color: red"><strong>Highlight (Top 10%)</strong></span>
<br/>[[paper](http://arxiv.org/abs/2312.01305)]  [[code](https://github.com/ubc-vision/vivid123)] [[project page](https://jgkwak95.github.io/ViVid-1-to-3/)]  
<br/>
<figure style="width: 50%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/Li_icassp.png" alt=""> </figure>
| <span style="font-size:110%">Towards Multi-domain Face Landmark Detection with Synthetic data from Diffusion model</span><br/>
<span style="font-size:90%; color: #808080">Yuanming Li, Gwantae Kim, , <span style="color: black">**Jeong-gi Kwak**</span>, Bonhwa Ku, Hanseok Ko</span><br/>
*IEEE International Conference on Acoustics, Speech and Signal Processing (**ICASSP**), 2024*
<br/>[[paper](https://arxiv.org/abs/2401.13191)]
<br/>
<figure style="width: 60%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/eccv2022_thumb.png" alt=""> </figure>| <span style="font-size:110%">Injecting 3D Perception of Controllable NeRF-GAN into StyleGAN for Editable Portrait Image Synthesis</span><br/>
<span style="font-size:90%">**Jeong-gi Kwak**, <span style="color: #808080">Yuanming Li, Dongsik Yoon,  Donghyeon Kim, David Han, Hanseok Ko </span></span><br/>
*European Conference on Computer Vision (**ECCV**), 2022*
<br/>[[paper](http://arxiv.org/abs/2207.10257)]  [[code](https://github.com/jgkwak95/SURF-GAN)]   [[project page](https://jgkwak95.github.io/surfgan/)]  
<span style="font-size:90%">**(2022.12) 2022 ETNews ICT Paper Awards sponsored by MSIT Korea** </span>
<br/>
<br/>
<figure style="width: 50%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/difai_thumb.png" alt=""> </figure>| <span style="font-size:110%">DIFAI: Diverse Facial Inpainting using StyleGAN Inversion</span><br/>
<span style="font-size:90%; color: #808080">Dongsik Yoon, <span style="color: black">**Jeong-gi Kwak**</span>,  Yuanming Li, David Han, Hanseok Ko</span><br/>*IEEE International Conference on Image Processing (**ICIP**), 2022*
<br/>[[paper](https://ieeexplore.ieee.org/document/9898012)]  
<br/>
<br/>
<figure style="width: 60%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/cvprw_thumb.png" alt=""> </figure>| <span style="font-size:110%">Generate and Edit Your Own Character in a Canonical View</span><br/>
<span style="font-size:90%; color: #808080"><span style="color: black">**Jeong-gi Kwak**</span>, Yuanming Li, Dongsik Yoon, David Han, Hanseok Ko</span><br/>
*IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (**CVPRW**), 2022*
<br/>[[paper](https://arxiv.org/abs/2205.02974)][[poster]({{ site.url }}{{ site.baseurl }}/files/cvpr_poster.pdf)]
<br/>
<br/>
<figure style="width: 60%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/augan.png" alt=""> </figure>| <span style="font-size:110%">Adverse Weather Image Translation with Asymmetric and Uncertainty-aware GAN</span><br/>
<span style="font-size:90%; color: #808080"><span style="color: black">**Jeong-gi Kwak**</span>, Youngsaeng Jin, Yuanming Li, Dongsik Yoon, Donghyeon Kim, Hanseok Ko</span><br/>
*British Machine Vision Conference (**BMVC**), 2021*
<br/>[[paper](https://www.bmvc2021-virtualconference.com/assets/papers/1443.pdf)]  [[code](https://github.com/jgkwak95/AU-GAN)]
<br/>
<br/>
<figure style="width: 50%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/yoon.png" alt=""> </figure>| <span style="font-size:110%">Reference Guided Image Inpainting using Facial Attributes</span><br/>
<span style="font-size:90%; color: #808080">Dongsik Yoon, <span style="color: black">**Jeong-gi Kwak**</span>,  Yuanming Li, David Han, Youngsaeng Jin, Hanseok Ko</span><br/>
*British Machine Vision Conference (**BMVC**), 2021*
<br/>[[paper](https://www.bmvc2021-virtualconference.com/assets/papers/1267.pdf)] [[code](https://github.com/Stillrot/RGINP)]
<br/>
<br/>
<figure style="width: 60%;margin-bottom: 0px"> <img src="{{ site.url }}{{ site.baseurl }}/images/cafe_gan.png" alt=""> </figure> | <span style="font-size:110%">CAFE-GAN: Arbitrary Face Attribute Editing with Complementary Attention Feature</span><br/>
<span style="font-size:90%; color: #808080"><span style="color: black">**Jeong-gi Kwak**</span>, David K. Han, Hanseok Ko</span><br/>
*European Conference on Computer Vision (**ECCV**), 2020*
<br/>[[paper](https://arxiv.org/pdf/2011.11900.pdf)] 
<br>
